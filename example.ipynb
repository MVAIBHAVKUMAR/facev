{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('test.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGFace2_DeepFace_weights_val-0.9034.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/swghosh/DeepFace/releases/download/weights-vggface2-2d-aligned/VGGFace2_DeepFace_weights_val-0.9034.h5.zip\n",
      "To: C:\\Users\\mvaib\\.deepface\\weights\\VGGFace2_DeepFace_weights_val-0.9034.h5.zip\n",
      "100%|██████████| 511M/511M [27:02<00:00, 315kB/s]   \n"
     ]
    }
   ],
   "source": [
    "#we can use this funtion to verify the faces in two images\n",
    "\n",
    "#and it will also return the facial area of the faces in the images\n",
    "result = DeepFace.verify(img1_path=\"test.jpeg\", img2_path=\"elon_musk.jpeg\", model_name=\"DeepFace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verified': True,\n",
       " 'distance': 0.22218670495760884,\n",
       " 'threshold': 0.23,\n",
       " 'model': 'DeepFace',\n",
       " 'detector_backend': 'opencv',\n",
       " 'similarity_metric': 'cosine',\n",
       " 'facial_areas': {'img1': {'x': 59, 'y': 47, 'w': 111, 'h': 111},\n",
       "  'img2': {'x': 76, 'y': 38, 'w': 122, 'h': 122}},\n",
       " 'time': 1756.2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect faces in the image\n",
    "detector = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "faces = detector.detectMultiScale(img, 1.3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': True, 'distance': 0.115636245159625, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 76, 'y': 38, 'w': 122, 'h': 122}, 'img2': {'x': 11, 'y': 10, 'w': 92, 'h': 92}}, 'time': 9.16}\n"
     ]
    }
   ],
   "source": [
    "# Perform face recognition on each detected face\n",
    "for (x, y, w, h) in faces:\n",
    "    face = img[y:y+h, x:x+w]\n",
    "    # Perform face recognition using DeepFace\n",
    "    result = DeepFace.verify(\"elon_musk.jpeg\", face)\n",
    "    # Print the result\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module deepface.DeepFace in deepface:\n",
      "\n",
      "NAME\n",
      "    deepface.DeepFace - # common dependencies\n",
      "\n",
      "FUNCTIONS\n",
      "    analyze(img_path, actions=('emotion', 'age', 'gender', 'race'), enforce_detection=True, detector_backend='opencv', align=True, silent=False)\n",
      "        This function analyzes facial attributes including age, gender, emotion and race.\n",
      "        In the background, analysis function builds convolutional neural network models to\n",
      "        classify age, gender, emotion and race of the input image.\n",
      "        \n",
      "        Parameters:\n",
      "                img_path: exact image path, numpy array (BGR) or base64 encoded image could be passed.\n",
      "                If source image has more than one face, then result will be size of number of faces\n",
      "                appearing in the image.\n",
      "        \n",
      "                actions (tuple): The default is ('age', 'gender', 'emotion', 'race'). You can drop\n",
      "                some of those attributes.\n",
      "        \n",
      "                enforce_detection (bool): The function throws exception if no face detected by default.\n",
      "                Set this to False if you don't want to get exception. This might be convenient for low\n",
      "                resolution images.\n",
      "        \n",
      "                detector_backend (string): set face detector backend to opencv, retinaface, mtcnn, ssd,\n",
      "                dlib or mediapipe.\n",
      "        \n",
      "                silent (boolean): disable (some) log messages\n",
      "        \n",
      "        Returns:\n",
      "                The function returns a list of dictionaries for each face appearing in the image.\n",
      "        \n",
      "                [\n",
      "                        {\n",
      "                                \"region\": {'x': 230, 'y': 120, 'w': 36, 'h': 45},\n",
      "                                \"age\": 28.66,\n",
      "                                \"dominant_gender\": \"Woman\",\n",
      "                                \"gender\": {\n",
      "                                        'Woman': 99.99407529830933,\n",
      "                                        'Man': 0.005928758764639497,\n",
      "                                }\n",
      "                                \"dominant_emotion\": \"neutral\",\n",
      "                                \"emotion\": {\n",
      "                                        'sad': 37.65260875225067,\n",
      "                                        'angry': 0.15512987738475204,\n",
      "                                        'surprise': 0.0022171278033056296,\n",
      "                                        'fear': 1.2489334680140018,\n",
      "                                        'happy': 4.609785228967667,\n",
      "                                        'disgust': 9.698561953541684e-07,\n",
      "                                        'neutral': 56.33133053779602\n",
      "                                }\n",
      "                                \"dominant_race\": \"white\",\n",
      "                                \"race\": {\n",
      "                                        'indian': 0.5480832420289516,\n",
      "                                        'asian': 0.7830780930817127,\n",
      "                                        'latino hispanic': 2.0677512511610985,\n",
      "                                        'black': 0.06337375962175429,\n",
      "                                        'middle eastern': 3.088453598320484,\n",
      "                                        'white': 93.44925880432129\n",
      "                                }\n",
      "                        }\n",
      "                ]\n",
      "    \n",
      "    build_model(model_name)\n",
      "        This function builds a deepface model\n",
      "        Parameters:\n",
      "                model_name (string): face recognition or facial attribute model\n",
      "                        VGG-Face, Facenet, OpenFace, DeepFace, DeepID for face recognition\n",
      "                        Age, Gender, Emotion, Race for facial attributes\n",
      "        \n",
      "        Returns:\n",
      "                built deepface model\n",
      "    \n",
      "    cli()\n",
      "        command line interface function will be offered in this block\n",
      "    \n",
      "    detectFace(img_path, target_size=(224, 224), detector_backend='opencv', enforce_detection=True, align=True)\n",
      "    \n",
      "    extract_faces(img_path, target_size=(224, 224), detector_backend='opencv', enforce_detection=True, align=True, grayscale=False)\n",
      "        This function applies pre-processing stages of a face recognition pipeline\n",
      "        including detection and alignment\n",
      "        \n",
      "        Parameters:\n",
      "                img_path: exact image path, numpy array (BGR) or base64 encoded image.\n",
      "                Source image can have many face. Then, result will be the size of number\n",
      "                of faces appearing in that source image.\n",
      "        \n",
      "                target_size (tuple): final shape of facial image. black pixels will be\n",
      "                added to resize the image.\n",
      "        \n",
      "                detector_backend (string): face detection backends are retinaface, mtcnn,\n",
      "                opencv, ssd or dlib\n",
      "        \n",
      "                enforce_detection (boolean): function throws exception if face cannot be\n",
      "                detected in the fed image. Set this to False if you do not want to get\n",
      "                an exception and run the function anyway.\n",
      "        \n",
      "                align (boolean): alignment according to the eye positions.\n",
      "        \n",
      "                grayscale (boolean): extracting faces in rgb or gray scale\n",
      "        \n",
      "        Returns:\n",
      "                list of dictionaries. Each dictionary will have facial image itself,\n",
      "                extracted area from the original image and confidence score.\n",
      "    \n",
      "    find(img_path, db_path, model_name='VGG-Face', distance_metric='cosine', enforce_detection=True, detector_backend='opencv', align=True, normalization='base', silent=False)\n",
      "        This function applies verification several times and find the identities in a database\n",
      "        \n",
      "        Parameters:\n",
      "                img_path: exact image path, numpy array (BGR) or based64 encoded image.\n",
      "                Source image can have many faces. Then, result will be the size of number of\n",
      "                faces in the source image.\n",
      "        \n",
      "                db_path (string): You should store some image files in a folder and pass the\n",
      "                exact folder path to this. A database image can also have many faces.\n",
      "                Then, all detected faces in db side will be considered in the decision.\n",
      "        \n",
      "                model_name (string): VGG-Face, Facenet, Facenet512, OpenFace, DeepFace, DeepID,\n",
      "                Dlib, ArcFace, SFace or Ensemble\n",
      "        \n",
      "                distance_metric (string): cosine, euclidean, euclidean_l2\n",
      "        \n",
      "                enforce_detection (bool): The function throws exception if a face could not be detected.\n",
      "                Set this to True if you don't want to get exception. This might be convenient for low\n",
      "                resolution images.\n",
      "        \n",
      "                detector_backend (string): set face detector backend to opencv, retinaface, mtcnn, ssd,\n",
      "                dlib or mediapipe\n",
      "        \n",
      "                silent (boolean): disable some logging and progress bars\n",
      "        \n",
      "        Returns:\n",
      "                This function returns list of pandas data frame. Each item of the list corresponding to\n",
      "                an identity in the img_path.\n",
      "    \n",
      "    represent(img_path, model_name='VGG-Face', enforce_detection=True, detector_backend='opencv', align=True, normalization='base')\n",
      "        This function represents facial images as vectors. The function uses convolutional neural\n",
      "        networks models to generate vector embeddings.\n",
      "        \n",
      "        Parameters:\n",
      "                img_path (string): exact image path. Alternatively, numpy array (BGR) or based64\n",
      "                encoded images could be passed. Source image can have many faces. Then, result will\n",
      "                be the size of number of faces appearing in the source image.\n",
      "        \n",
      "                model_name (string): VGG-Face, Facenet, Facenet512, OpenFace, DeepFace, DeepID, Dlib,\n",
      "                ArcFace, SFace\n",
      "        \n",
      "                enforce_detection (boolean): If no face could not be detected in an image, then this\n",
      "                function will return exception by default. Set this to False not to have this exception.\n",
      "                This might be convenient for low resolution images.\n",
      "        \n",
      "                detector_backend (string): set face detector backend to opencv, retinaface, mtcnn, ssd,\n",
      "                dlib or mediapipe\n",
      "        \n",
      "                align (boolean): alignment according to the eye positions.\n",
      "        \n",
      "                normalization (string): normalize the input image before feeding to model\n",
      "        \n",
      "        Returns:\n",
      "                Represent function returns a list of object with multidimensional vector (embedding).\n",
      "                The number of dimensions is changing based on the reference model.\n",
      "                E.g. FaceNet returns 128 dimensional vector; VGG-Face returns 2622 dimensional vector.\n",
      "    \n",
      "    stream(db_path='', model_name='VGG-Face', detector_backend='opencv', distance_metric='cosine', enable_face_analysis=True, source=0, time_threshold=5, frame_threshold=5)\n",
      "        This function applies real time face recognition and facial attribute analysis\n",
      "        \n",
      "        Parameters:\n",
      "                db_path (string): facial database path. You should store some .jpg files in this folder.\n",
      "        \n",
      "                model_name (string): VGG-Face, Facenet, Facenet512, OpenFace, DeepFace, DeepID, Dlib,\n",
      "                ArcFace, SFace\n",
      "        \n",
      "                detector_backend (string): opencv, retinaface, mtcnn, ssd, dlib or mediapipe\n",
      "        \n",
      "                distance_metric (string): cosine, euclidean, euclidean_l2\n",
      "        \n",
      "                enable_facial_analysis (boolean): Set this to False to just run face recognition\n",
      "        \n",
      "                source: Set this to 0 for access web cam. Otherwise, pass exact video path.\n",
      "        \n",
      "                time_threshold (int): how many second analyzed image will be displayed\n",
      "        \n",
      "                frame_threshold (int): how many frames required to focus on face\n",
      "    \n",
      "    verify(img1_path, img2_path, model_name='VGG-Face', detector_backend='opencv', distance_metric='cosine', enforce_detection=True, align=True, normalization='base')\n",
      "        This function verifies an image pair is same person or different persons. In the background,\n",
      "        verification function represents facial images as vectors and then calculates the similarity\n",
      "        between those vectors. Vectors of same person images should have more similarity (or less\n",
      "        distance) than vectors of different persons.\n",
      "        \n",
      "        Parameters:\n",
      "                img1_path, img2_path: exact image path as string. numpy array (BGR) or based64 encoded\n",
      "                images are also welcome. If one of pair has more than one face, then we will compare the\n",
      "                face pair with max similarity.\n",
      "        \n",
      "                model_name (str): VGG-Face, Facenet, Facenet512, OpenFace, DeepFace, DeepID, Dlib\n",
      "                , ArcFace and SFace\n",
      "        \n",
      "                distance_metric (string): cosine, euclidean, euclidean_l2\n",
      "        \n",
      "                enforce_detection (boolean): If no face could not be detected in an image, then this\n",
      "                function will return exception by default. Set this to False not to have this exception.\n",
      "                This might be convenient for low resolution images.\n",
      "        \n",
      "                detector_backend (string): set face detector backend to opencv, retinaface, mtcnn, ssd,\n",
      "                dlib or mediapipe\n",
      "        \n",
      "        Returns:\n",
      "                Verify function returns a dictionary.\n",
      "        \n",
      "                {\n",
      "                        \"verified\": True\n",
      "                        , \"distance\": 0.2563\n",
      "                        , \"max_threshold_to_verify\": 0.40\n",
      "                        , \"model\": \"VGG-Face\"\n",
      "                        , \"similarity_metric\": \"cosine\"\n",
      "                        , 'facial_areas': {\n",
      "                                'img1': {'x': 345, 'y': 211, 'w': 769, 'h': 769},\n",
      "                                'img2': {'x': 318, 'y': 534, 'w': 779, 'h': 779}\n",
      "                        }\n",
      "                        , \"time\": 2\n",
      "                }\n",
      "\n",
      "DATA\n",
      "    model_obj = {'VGG-Face': <keras.engine.functional.Functional object>}\n",
      "    tf_version = 2\n",
      "\n",
      "FILE\n",
      "    c:\\users\\mvaib\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\deepface\\deepface.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(DeepFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
